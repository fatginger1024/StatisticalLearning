---
title: "Assignment Week 4"
output:
  pdf_document: default
  html_document: default
---

### Name: Qing Zhou

### ULCN: s2501597

```{r label = preps, echo=FALSE}
rm(list=ls()) #clears your workspace
```

### Exercise 1

We are interested in the *inverse* of the function $f(x)=x\log(x)$. Unfortunately this inverse has no closed-form solution, so we'll need to re2,sort to numerical methods.

**(a)** Write a function that implements the inverse $f^{-1}$ of $f$ using the bisection method. It only has to work for the part of the function where $x\ge e$, where $e=2.71828\ldots$ To get suitable starting values, you can use the fact that if $x\log(x)=y$, then $x\in(e,y]$. Test your function by checking that the calculation of $f^{-1}(f(5))$ yields a result (close to) $5$. How many iterations are needed to obtain an accuracy of $10^{-6}$ in this test?

```{r}
y<-seq(exp(1)*log(exp(1)),5*log(5),1e-3)
f<-function(x,y){x*log(x)-y}
bisect<-function(f,y,eps){
  N<-length(y)
  xval<-rep(0,N)
  iters<-rep(0,N)
  for(i in 1:N){
    yval<-y[i]
    a<-1
    b<-yval+3
    count<-0
    while(b-a>eps){
      count<-count+1
      mid<-(a+b)/2
      if (f(a,yval)*f(mid,yval)<0) b<-mid else a<-mid
      xval[i]<-mid
    }
    iters[i]<-count
  }
  lst<-list("xval"=xval,"iters"=iters)
  return(lst)
}
x<-bisect(f,y,1e-6)$xval
iter<-bisect(f,y,1e-6)$iters
plot(y,x,col=2,type='l')
cat('The minimum of x is: ',min(x),"\n")
cat('The maximum of x is: ',max(x),"\n")
cat("Num of iterations needed for 1e-6 accuracy for test: ",iter[length(iter)])
```

**(b)** Implement the method of Newton-Raphson and test it for this same problem, i.e. to calculate the inverse of $f$ at $f(5)$. (The derivative of $f$ is $f'(x)=\log(x)+1$.) Choose starting value 10. How many iterations are needed to obtain accuracy $10^{-6}$?
```{r}
y<-seq(exp(1)*log(exp(1)),5*log(5),1e-3)
f<-function(x,y){x*log(x)-y}
fprime<-function(x){log(x)+1}
newton<-function(f,fprime,y,ct){
  N<-length(y)
  xval<-rep(0,N)
  iters<-rep(0,N)
  for(i in 1:N){
    yval<-y[i]
    x0<-yval
    xval[i]<-x0
    count<-0
    while(abs(xval[i]-f(xval[i],yval)/fprime(xval[i])-xval[i])>1e-6){
      count<-count+1
      xval[i]<-xval[i]-f(xval[i],yval)/fprime(xval[i])
    }
    iters[i]<-count
  }
  lst<-list("xval"=xval,"iters"=iters)
  return(lst)
}
x<-newton(f,fprime,y,30)$xval
iter<-newton(f,fprime,y,30)$iters
plot(y,x,col=2,type='l')
cat('The minimum of x is: ',min(x),"\n")
cat('The maximum of x is: ',max(x),"\n")
cat("Num of iterations needed for 1e-6 accuracy for test: ",iter[length(iter)])
```
Be careful! We are considering the accuracy of the estimate of $x$, so we want to know after how many iterations $x$ is sufficiently close to $5$, *not* after how many iterations $f(x)$ is sufficiently close to $f(5)$.

Exercise 2
--------------
Look at the dataset `swiss` that's built into R. We wish to predict the column `Infant.Mortality` based on the other columns using ordinary least squares (OLS). The following code uses the machinery built into R to fit the parameters.

```{r}
attach(swiss)
model <- lm(Infant.Mortality ~ Fertility + Agriculture + Examination + Education + Catholic)
model$coefficients
detach(swiss)
```

The model predicts `Infant.Mortality` of each row as the inner product of the values in the other columns with the found coefficients, where the "intercept" coefficient applies to an imaginary row whose value is 1 everywhere.

We can use techniques from linear algebra to find the OLS solution ourselves:

**(a)** Define a matrix or data frame `X` whose first column is all ones, and whose subsequent columns are the columns for all the predictor variables in `swiss`. (That is, all columns from `swiss` except for `Infant.Mortality`.) Also define a vector or data frame `Y` that contains the `Infant.Mortality` column from the data set. Then fit the OLS coefficients $\hat\beta$ by hand, by using the formula

$$\hat\beta=(X^TX)^{-1}X^TY.$$

Check that the coefficients you find match the ones from the linear model.

*Hint: `solve(A)` computes the inverse of a matrix A. For an overview of matrix operations see https://www.statmethods.net/advstats/matrix.html*
```{r}
attach(swiss)
N<-length(swiss$Fertility)
nelements<-N*6
matX<-matrix(1:nelements,ncol=6)
matX[,1]<-c(rep(1,N))
matX[,2]<-swiss$Fertility
matX[,3]<-swiss$Agriculture
matX[,4]<-swiss$Examination
matX[,5]<-swiss$Education
matX[,6]<-swiss$Catholic
matY<-matrix(1:N)
matY[,1]<-swiss$Infant.Mortality
multX<-t(matX)%*%matX
s<-svd(multX)
U<-s$u
invD<-diag(1/s$d)
V<-s$v
invX<-V%*%invD%*%t(U)
beta<-invX%*%t(matX)%*%matY
cat("The coefficients found are: ","\n",beta)
```

**(b)** Instead of using the algebraic solution, now find the OLS coefficients by minimising the mean of squared errors (MSE) with respect to the coefficients $\beta$ using the built-in optimisation method `nlm`. Again check that the coefficients match what we found before. Note: the squared error for a row $i$ is the squared difference between $Y_i$ and the inner product between $X_i$ and $\beta$. The mean squared error is the average over all rows.
```{r}
x<-matX
p0<-c(rep(1,6))
f<-function(beta){
  sqe<-(x%*%beta-matY)^2
  return(sum(sqe)/6)
}
coef<-nlm(f,p0)$estimate
cat("The coefficients found using MSE are: ","\n",coef)
```

**(c)** Optimisation may have seemed overkill here, but it grants additional flexibility, because we can now easily change what function we are optimising. Instead of minimising the MSE, we can for example implement Lasso regression by adding L1-regularisation: find the coefficients that minimizing the function $\text{MSE}(\beta)+\lambda\|\beta\|_1$, where $\|\beta\|_1$ is the L1-norm: the sum of the absolute values of the coefficients. Use $\lambda=0.02$.

```{r}
x<-matX
p0<-c(rep(1,6))
f<-function(beta){
  sqe<-(x%*%beta-matY)^2
  lambda<-.02
  L1_norm<-sum(abs(beta))
  func<-sum(sqe)/6+lambda*L1_norm
  return(func)
}
coef<-nlm(f,p0)$estimate
cat("The coefficients found using MSE L1norm are: ","\n",coef)
```
